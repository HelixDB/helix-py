{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b4f5ec",
   "metadata": {},
   "source": [
    "# Helix Chunking\n",
    "\n",
    "Uses Chonkie's chunker under the hood. You can read which Chonkie chunker will work best for your use case at https://docs.chonkie.ai/python-sdk/chunkers/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b89d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9848b0bf",
   "metadata": {},
   "source": [
    "### Sample Text for Single Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea4ed391",
   "metadata": {},
   "outputs": [],
   "source": [
    "massive_text_blob = \"\"\"\n",
    "This is a massive text blob that we want to chunk into smaller pieces for processing. It contains multiple sentences and paragraphs that need to be divided appropriately to maintain context while fitting within token limits. When working with large documents, it is important to ensure that each chunk maintains enough context for downstream tasks, such as retrieval or summarization. Chunking strategies can vary depending on the use case, but the goal is always to balance context preservation with processing efficiency.\n",
    "\n",
    "The chunker should handle overlaps properly to ensure no important information is lost at chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that both chunks retain the full meaning of the text. This is especially important in applications like document question answering, where missing a single sentence could lead to incorrect answers. Additionally, chunkers may need to account for different languages, code blocks, or special formatting, which can add complexity to the chunking process.\n",
    "\n",
    "This example demonstrates how the token chunker works with a realistic text sample that would be common in document processing and RAG (Retrieval-Augmented Generation) applications. The chunks will be created with specified token limits and overlap settings to optimize for both comprehension and processing efficiency. Each chunk will contain metadata about its position in the original text and token count for further processing. By using a robust chunking strategy, we can ensure that downstream models receive high-quality, context-rich input, improving the overall performance of NLP pipelines and applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445216f",
   "metadata": {},
   "source": [
    "### Sample Text List for Batch Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2886928",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"First document to chunk with some content for testing.\",\n",
    "    \"Second document with different content for batch processing.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5cf178",
   "metadata": {},
   "source": [
    "### Sample Text For Code Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc5224a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_sample = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, Chonkie!\")\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self.value = 42\n",
    "\"\"\"\n",
    "\n",
    "code_samples = [\n",
    "    \"def func1():\\n    pass\",\n",
    "    \"const x = 10;\\nfunction add(a, b) { return a + b; }\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0861b498",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c1426e",
   "metadata": {},
   "source": [
    "#### Printing out Chunks from Single Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d662917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_chunks(chunks, chunker_name):\n",
    "    print(f\"\\n=== {chunker_name} - Single Text ===\")\n",
    "    print(f\"Created {len(chunks)} chunks:\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"\\nChunk {i+1}:\")\n",
    "        print(f\"  Text: {chunk.text}\")\n",
    "        print(f\"  Start: {chunk.start_index}\")\n",
    "        print(f\"  End: {chunk.end_index}\")\n",
    "        print(f\"  Tokens: {chunk.token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec82218",
   "metadata": {},
   "source": [
    "#### Printing out Chunks from Batch Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df4ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch_chunks(batch_chunks, chunker_name):\n",
    "    print(f\"\\n=== {chunker_name} - Batch Text ===\")\n",
    "    for doc_idx, doc_chunks in enumerate(batch_chunks):\n",
    "        print(f\"\\nDocument {doc_idx + 1} ({len(doc_chunks)} chunks):\")\n",
    "        for chunk_idx, chunk in enumerate(doc_chunks):\n",
    "            text_preview = chunk.text\n",
    "            print(f\"  Chunk {chunk_idx + 1}: {text_preview} (tokens: {chunk.token_count})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c01de2",
   "metadata": {},
   "source": [
    "### Token Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eded7b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Token Chunker - Single Text ===\n",
      "Created 1 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "  Text: \n",
      "This is a massive text blob that we want to chunk into smaller pieces for processing. It contains multiple sentences and paragraphs that need to be divided appropriately to maintain context while fitting within token limits. When working with large documents, it is important to ensure that each chunk maintains enough context for downstream tasks, such as retrieval or summarization. Chunking strategies can vary depending on the use case, but the goal is always to balance context preservation with processing efficiency.\n",
      "\n",
      "The chunker should handle overlaps properly to ensure no important information is lost at chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that both chunks retain the full meaning of the text. This is especially important in applications like document question answering, where missing a single sentence could lead to incorrect answers. Additionally, chunkers may need to account for different languages, code blocks, or special formatting, which can add complexity to the chunking process.\n",
      "\n",
      "This example demonstrates how the token chunker works with a realistic text sample that would be common in document processing and RAG (Retrieval-Augmented Generation) applications. The chunks will be created with specified token limits and overlap settings to optimize for both comprehension and processing efficiency. Each chunk will contain metadata about its position in the original text and token count for further processing. By using a robust chunking strategy, we can ensure that downstream models receive high-quality, context-rich input, improving the overall performance of NLP pipelines and applications.\n",
      "\n",
      "  Start: 0\n",
      "  End: 1681\n",
      "  Tokens: 1681\n"
     ]
    }
   ],
   "source": [
    "chunks = helix.Chunk.token_chunk(massive_text_blob)\n",
    "print_chunks(chunks, \"Token Chunker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5c5d452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ¦› choooooooooooooooooooonk 100% â€¢ 2/2 batches chunked [00:00<00:00, 4040.76batch/s] ðŸŒ±"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Token Chunker - Batch Text ===\n",
      "\n",
      "Document 1 (1 chunks):\n",
      "  Chunk 1: First document to chunk with some content for testing. (tokens: 54)\n",
      "\n",
      "Document 2 (1 chunks):\n",
      "  Chunk 1: Second document with different content for batch processing. (tokens: 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_chunks = helix.Chunk.token_chunk(texts)\n",
    "print_batch_chunks(batch_chunks, \"Token Chunker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad93b5a2",
   "metadata": {},
   "source": [
    "### Sentence Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cb5afd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sentence Chunker - Single Text ===\n",
      "Created 1 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "  Text: \n",
      "This is a massive text blob that we want to chunk into smaller pieces for processing. It contains multiple sentences and paragraphs that need to be divided appropriately to maintain context while fitting within token limits. When working with large documents, it is important to ensure that each chunk maintains enough context for downstream tasks, such as retrieval or summarization. Chunking strategies can vary depending on the use case, but the goal is always to balance context preservation with processing efficiency.\n",
      "\n",
      "The chunker should handle overlaps properly to ensure no important information is lost at chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that both chunks retain the full meaning of the text. This is especially important in applications like document question answering, where missing a single sentence could lead to incorrect answers. Additionally, chunkers may need to account for different languages, code blocks, or special formatting, which can add complexity to the chunking process.\n",
      "\n",
      "This example demonstrates how the token chunker works with a realistic text sample that would be common in document processing and RAG (Retrieval-Augmented Generation) applications. The chunks will be created with specified token limits and overlap settings to optimize for both comprehension and processing efficiency. Each chunk will contain metadata about its position in the original text and token count for further processing. By using a robust chunking strategy, we can ensure that downstream models receive high-quality, context-rich input, improving the overall performance of NLP pipelines and applications.\n",
      "\n",
      "  Start: 0\n",
      "  End: 1681\n",
      "  Tokens: 1681\n"
     ]
    }
   ],
   "source": [
    "chunks = helix.Chunk.sentence_chunk(massive_text_blob)\n",
    "print_chunks(chunks, \"Sentence Chunker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac704576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ¦› choooooooooooooooooooonk 100% â€¢ 2/2 docs chunked [00:00<00:00, 11.63doc/s] ðŸŒ±"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sentence Chunker - Batch Text ===\n",
      "\n",
      "Document 1 (1 chunks):\n",
      "  Chunk 1: First document to chunk with some content for testing. (tokens: 54)\n",
      "\n",
      "Document 2 (1 chunks):\n",
      "  Chunk 1: Second document with different content for batch processing. (tokens: 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_chunks = helix.Chunk.sentence_chunk(texts)\n",
    "print_batch_chunks(batch_chunks, \"Sentence Chunker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4314d2",
   "metadata": {},
   "source": [
    "### Recursive Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe9f47d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Recursive Chunker - Single Text ===\n",
      "Created 1 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "  Text: \n",
      "This is a massive text blob that we want to chunk into smaller pieces for processing. It contains multiple sentences and paragraphs that need to be divided appropriately to maintain context while fitting within token limits. When working with large documents, it is important to ensure that each chunk maintains enough context for downstream tasks, such as retrieval or summarization. Chunking strategies can vary depending on the use case, but the goal is always to balance context preservation with processing efficiency.\n",
      "\n",
      "The chunker should handle overlaps properly to ensure no important information is lost at chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that both chunks retain the full meaning of the text. This is especially important in applications like document question answering, where missing a single sentence could lead to incorrect answers. Additionally, chunkers may need to account for different languages, code blocks, or special formatting, which can add complexity to the chunking process.\n",
      "\n",
      "This example demonstrates how the token chunker works with a realistic text sample that would be common in document processing and RAG (Retrieval-Augmented Generation) applications. The chunks will be created with specified token limits and overlap settings to optimize for both comprehension and processing efficiency. Each chunk will contain metadata about its position in the original text and token count for further processing. By using a robust chunking strategy, we can ensure that downstream models receive high-quality, context-rich input, improving the overall performance of NLP pipelines and applications.\n",
      "\n",
      "  Start: 0\n",
      "  End: 1681\n",
      "  Tokens: 1681\n"
     ]
    }
   ],
   "source": [
    "chunks = helix.Chunk.recursive_chunk(massive_text_blob)\n",
    "print_chunks(chunks, \"Recursive Chunker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4633ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ¦› choooooooooooooooooooonk 100% â€¢ 2/2 docs chunked [00:00<00:00, 13.45doc/s] ðŸŒ±"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Recursive Chunker - Batch Text ===\n",
      "\n",
      "Document 1 (1 chunks):\n",
      "  Chunk 1: First document to chunk with some content for testing. (tokens: 54)\n",
      "\n",
      "Document 2 (1 chunks):\n",
      "  Chunk 1: Second document with different content for batch processing. (tokens: 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_chunks = helix.Chunk.recursive_chunk(texts)\n",
    "print_batch_chunks(batch_chunks, \"Recursive Chunker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b328d9",
   "metadata": {},
   "source": [
    "### Code Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09bfcc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Code Chunker - Single Text ===\n",
      "Created 1 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "  Text: \n",
      "def hello_world():\n",
      "    print(\"Hello, Chonkie!\")\n",
      "\n",
      "class MyClass:\n",
      "    def __init__(self):\n",
      "        self.value = 42\n",
      "\n",
      "  Start: 0\n",
      "  End: 113\n",
      "  Tokens: 109\n"
     ]
    }
   ],
   "source": [
    "chunks = helix.Chunk.code_chunk(code_sample, language=\"python\")\n",
    "print_chunks(chunks, \"Code Chunker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca46a51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ¦› choooooooooooooooooooonk 100% â€¢ 2/2 docs chunked [00:00<00:00, 3210.34doc/s] ðŸŒ±"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Code Chunker - Batch Text ===\n",
      "\n",
      "Document 1 (1 chunks):\n",
      "  Chunk 1: def func1():\n",
      "    pass (tokens: 21)\n",
      "\n",
      "Document 2 (1 chunks):\n",
      "  Chunk 1: const x = 10;\n",
      "function add(a, b) { return a + b; } (tokens: 49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_chunks = helix.Chunk.code_chunk(code_samples, language=\"python\")\n",
    "print_batch_chunks(batch_chunks, \"Code Chunker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceca787b",
   "metadata": {},
   "source": [
    "### Semantic Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceea8c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/putt/Documents/Github/helix-py/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Semantic Chunker - Single Text ===\n",
      "Created 7 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "  Text: \n",
      "This is a massive text blob that we want to chunk into smaller pieces for processing. It contains multiple sentences and paragraphs that need to be divided appropriately to maintain context while fitting within token limits. When working with large documents, it is important to ensure that each chunk maintains enough context for downstream tasks, such as retrieval or summarization. Chunking strategies can vary depending on the use case, but the goal is always to balance context preservation with processing efficiency.\n",
      "  Start: 0\n",
      "  End: 524\n",
      "  Tokens: 94\n",
      "\n",
      "Chunk 2:\n",
      "  Text: \n",
      "\n",
      "The chunker should handle overlaps properly to ensure no important information is lost at chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that both chunks retain the full meaning of the text.\n",
      "  Start: 524\n",
      "  End: 763\n",
      "  Tokens: 45\n",
      "\n",
      "Chunk 3:\n",
      "  Text:  This is especially important in applications like document question answering, where missing a single sentence could lead to incorrect answers.\n",
      "  Start: 763\n",
      "  End: 907\n",
      "  Tokens: 22\n",
      "\n",
      "Chunk 4:\n",
      "  Text:  Additionally, chunkers may need to account for different languages, code blocks, or special formatting, which can add complexity to the chunking process.\n",
      "  Start: 907\n",
      "  End: 1061\n",
      "  Tokens: 30\n",
      "\n",
      "Chunk 5:\n",
      "  Text: \n",
      "\n",
      "This example demonstrates how the token chunker works with a realistic text sample that would be common in document processing and RAG (Retrieval-Augmented Generation) applications.\n",
      "  Start: 1061\n",
      "  End: 1244\n",
      "  Tokens: 31\n",
      "\n",
      "Chunk 6:\n",
      "  Text:  The chunks will be created with specified token limits and overlap settings to optimize for both comprehension and processing efficiency. Each chunk will contain metadata about its position in the original text and token count for further processing.\n",
      "  Start: 1244\n",
      "  End: 1495\n",
      "  Tokens: 42\n",
      "\n",
      "Chunk 7:\n",
      "  Text:  By using a robust chunking strategy, we can ensure that downstream models receive high-quality, context-rich input, improving the overall performance of NLP pipelines and applications.\n",
      "\n",
      "  Start: 1495\n",
      "  End: 1681\n",
      "  Tokens: 36\n"
     ]
    }
   ],
   "source": [
    "chunks = helix.Chunk.semantic_chunk(massive_text_blob)\n",
    "print_chunks(chunks, \"Semantic Chunker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "174632c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ¦› choooooooooooooooooooonk 100% â€¢ 2/2 docs chunked [00:00<00:00, 3343.41doc/s] ðŸŒ±"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Semantic Chunker - Batch Text ===\n",
      "\n",
      "Document 1 (1 chunks):\n",
      "  Chunk 1: First document to chunk with some content for testing. (tokens: 10)\n",
      "\n",
      "Document 2 (1 chunks):\n",
      "  Chunk 1: Second document with different content for batch processing. (tokens: 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_chunks = helix.Chunk.semantic_chunk(texts)\n",
    "print_batch_chunks(batch_chunks, \"Semantic Chunker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4363d271",
   "metadata": {},
   "source": [
    "### SDPM Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2e2b2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SDPM Chunker - Single Text ===\n",
      "Created 7 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "  Text: \n",
      "This is a massive text blob that we want to chunk into smaller pieces for processing. It contains multiple sentences and paragraphs that need to be divided appropriately to maintain context while fitting within token limits. When working with large documents, it is important to ensure that each chunk maintains enough context for downstream tasks, such as retrieval or summarization. Chunking strategies can vary depending on the use case, but the goal is always to balance context preservation with processing efficiency.\n",
      "  Start: 0\n",
      "  End: 524\n",
      "  Tokens: 94\n",
      "\n",
      "Chunk 2:\n",
      "  Text: \n",
      "\n",
      "The chunker should handle overlaps properly to ensure no important information is lost at chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that both chunks retain the full meaning of the text.\n",
      "  Start: 524\n",
      "  End: 763\n",
      "  Tokens: 45\n",
      "\n",
      "Chunk 3:\n",
      "  Text:  This is especially important in applications like document question answering, where missing a single sentence could lead to incorrect answers.\n",
      "  Start: 763\n",
      "  End: 907\n",
      "  Tokens: 22\n",
      "\n",
      "Chunk 4:\n",
      "  Text:  Additionally, chunkers may need to account for different languages, code blocks, or special formatting, which can add complexity to the chunking process.\n",
      "  Start: 907\n",
      "  End: 1061\n",
      "  Tokens: 30\n",
      "\n",
      "Chunk 5:\n",
      "  Text: \n",
      "\n",
      "This example demonstrates how the token chunker works with a realistic text sample that would be common in document processing and RAG (Retrieval-Augmented Generation) applications.\n",
      "  Start: 1061\n",
      "  End: 1244\n",
      "  Tokens: 31\n",
      "\n",
      "Chunk 6:\n",
      "  Text:  The chunks will be created with specified token limits and overlap settings to optimize for both comprehension and processing efficiency. Each chunk will contain metadata about its position in the original text and token count for further processing.\n",
      "  Start: 1244\n",
      "  End: 1495\n",
      "  Tokens: 42\n",
      "\n",
      "Chunk 7:\n",
      "  Text:  By using a robust chunking strategy, we can ensure that downstream models receive high-quality, context-rich input, improving the overall performance of NLP pipelines and applications.\n",
      "\n",
      "  Start: 1495\n",
      "  End: 1681\n",
      "  Tokens: 36\n"
     ]
    }
   ],
   "source": [
    "chunks = helix.Chunk.sdp_chunk(massive_text_blob)\n",
    "print_chunks(chunks, \"SDPM Chunker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a49d6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ¦› choooooooooooooooooooonk 100% â€¢ 2/2 docs chunked [00:00<00:00, 4527.04doc/s] ðŸŒ±"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SDPM Chunker - Batch Text ===\n",
      "\n",
      "Document 1 (1 chunks):\n",
      "  Chunk 1: First document to chunk with some content for testing. (tokens: 10)\n",
      "\n",
      "Document 2 (1 chunks):\n",
      "  Chunk 1: Second document with different content for batch processing. (tokens: 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_chunks = helix.Chunk.sdp_chunk(texts)\n",
    "print_batch_chunks(batch_chunks, \"SDPM Chunker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c8cab",
   "metadata": {},
   "source": [
    "### Late Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad30713e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (300 > 256). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Late Chunker - Single Text ===\n",
      "Created 1 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "  Text: \n",
      "This is a massive text blob that we want to chunk into smaller pieces for processing. It contains multiple sentences and paragraphs that need to be divided appropriately to maintain context while fitting within token limits. When working with large documents, it is important to ensure that each chunk maintains enough context for downstream tasks, such as retrieval or summarization. Chunking strategies can vary depending on the use case, but the goal is always to balance context preservation with processing efficiency.\n",
      "\n",
      "The chunker should handle overlaps properly to ensure no important information is lost at chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that both chunks retain the full meaning of the text. This is especially important in applications like document question answering, where missing a single sentence could lead to incorrect answers. Additionally, chunkers may need to account for different languages, code blocks, or special formatting, which can add complexity to the chunking process.\n",
      "\n",
      "This example demonstrates how the token chunker works with a realistic text sample that would be common in document processing and RAG (Retrieval-Augmented Generation) applications. The chunks will be created with specified token limits and overlap settings to optimize for both comprehension and processing efficiency. Each chunk will contain metadata about its position in the original text and token count for further processing. By using a robust chunking strategy, we can ensure that downstream models receive high-quality, context-rich input, improving the overall performance of NLP pipelines and applications.\n",
      "\n",
      "  Start: 0\n",
      "  End: 1681\n",
      "  Tokens: 302\n"
     ]
    }
   ],
   "source": [
    "chunks = helix.Chunk.late_chunk(massive_text_blob)\n",
    "print_chunks(chunks, \"Late Chunker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "065c65e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ¦› choooooooooooooooooooonk 100% â€¢ 2/2 docs chunked [00:00<00:00, 24.91doc/s] ðŸŒ±"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Late Chunker - Batch Text ===\n",
      "\n",
      "Document 1 (1 chunks):\n",
      "  Chunk 1: First document to chunk with some content for testing. (tokens: 12)\n",
      "\n",
      "Document 2 (1 chunks):\n",
      "  Chunk 1: Second document with different content for batch processing. (tokens: 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_chunks = helix.Chunk.late_chunk(texts)\n",
    "print_batch_chunks(batch_chunks, \"Late Chunker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2960f2",
   "metadata": {},
   "source": [
    "### Neural Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76033b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Neural Chunker - Single Text ===\n",
      "Created 8 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "  Text: \n",
      "\n",
      "  Start: 0\n",
      "  End: 1\n",
      "  Tokens: 1\n",
      "\n",
      "Chunk 2:\n",
      "  Text: This is a massive text blob that we want to chunk into smaller pieces for processing. It contains multiple sentences and paragraphs that need to be divided appropriately to maintain context while fitting within token limits.\n",
      "  Start: 1\n",
      "  End: 225\n",
      "  Tokens: 38\n",
      "\n",
      "Chunk 3:\n",
      "  Text:  When working with large documents, it is important to ensure that each chunk maintains enough context for downstream tasks, such as retrieval or summarization. Chunking strategies can vary depending on the use case, but the goal is always to balance context preservation with processing efficiency.\n",
      "\n",
      "  Start: 225\n",
      "  End: 525\n",
      "  Tokens: 54\n",
      "\n",
      "Chunk 4:\n",
      "  Text: \n",
      "The chunker should handle overlaps properly to ensure no important information is lost at chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that both chunks retain the full meaning of the text.\n",
      "  Start: 525\n",
      "  End: 763\n",
      "  Tokens: 45\n",
      "\n",
      "Chunk 5:\n",
      "  Text:  This is especially important in applications like document question answering, where missing a single sentence could lead to incorrect answers. Additionally, chunkers may need to account for different languages, code blocks, or special formatting, which can add complexity to the chunking process.\n",
      "\n",
      "\n",
      "  Start: 763\n",
      "  End: 1063\n",
      "  Tokens: 52\n",
      "\n",
      "Chunk 6:\n",
      "  Text: This example demonstrates how the token chunker works with a realistic text sample that would be common in document processing and RAG (Retrieval-Augmented Generation) applications.\n",
      "  Start: 1063\n",
      "  End: 1244\n",
      "  Tokens: 35\n",
      "\n",
      "Chunk 7:\n",
      "  Text:  The chunks will be created with specified token limits and overlap settings to optimize for both comprehension and processing efficiency. Each chunk will contain metadata about its position in the original text and token count for further processing\n",
      "  Start: 1244\n",
      "  End: 1494\n",
      "  Tokens: 39\n",
      "\n",
      "Chunk 8:\n",
      "  Text: . By using a robust chunking strategy, we can ensure that downstream models receive high-quality, context-rich input, improving the overall performance of NLP pipelines and applications.\n",
      "\n",
      "  Start: 1494\n",
      "  End: 1681\n",
      "  Tokens: 37\n"
     ]
    }
   ],
   "source": [
    "chunks = helix.Chunk.neural_chunk(massive_text_blob)\n",
    "print_chunks(chunks, \"Neural Chunker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bacacfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "ðŸ¦› choooooooooooooooooooonk 100% â€¢ 2/2 docs chunked [00:00<00:00, 14.30doc/s] ðŸŒ±"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Neural Chunker - Batch Text ===\n",
      "\n",
      "Document 1 (1 chunks):\n",
      "  Chunk 1: First document to chunk with some content for testing. (tokens: 10)\n",
      "\n",
      "Document 2 (1 chunks):\n",
      "  Chunk 1: Second document with different content for batch processing. (tokens: 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_chunks = helix.Chunk.neural_chunk(texts)\n",
    "print_batch_chunks(batch_chunks, \"Neural Chunker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc0be1",
   "metadata": {},
   "source": [
    "### Slumber Chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505e26b",
   "metadata": {},
   "source": [
    "You need to set an Gemini API key in your env to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36fdf7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3fdc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ¦› choooooooooooooooooooonk 100% â€¢ 36/36 splits processed [00:41<00:00,  1.17s/split] ðŸŒ±"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Slumber Chunker - Single Text ===\n",
      "Created 3 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "  Text: \n",
      "This is a massive text blob that we want to chunk into smaller pieces for processing. Itcontainsmultiplesentencesandparagraphsthatneedtobedividedappropriatelytomaintaincontextwhilefittingwithintokenlimits.When working with large documents, it is important to ensure that each chunk maintains enough context for downstream tasks, such as retrieval or summarization. Chunking strategies can vary depending on the use case, but the goal is always to balance context preservation with processing efficiency.\n",
      "\n",
      "The chunker should handle overlaps properly to ensure no important information is lost at chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that both chunks retain the full meaning of the text. This is especially important in applications like document question answering, where missing a single sentence could lead to incorrect answers. \n",
      "  Start: 0\n",
      "  End: 908\n",
      "  Tokens: 888\n",
      "\n",
      "Chunk 2:\n",
      "  Text: Additionally, chunkers may need to account for different languages, code blocks, or special formatting, which can add complexity to the chunking process.\n",
      "\n",
      "  Start: 908\n",
      "  End: 1062\n",
      "  Tokens: 154\n",
      "\n",
      "Chunk 3:\n",
      "  Text: \n",
      "ThisexampledemonstrateshowthetokenchunkerworkswitharealistictextsamplethatwouldbecommonindocumentprocessingandRAG(Retrieval-Augmented Generation) applications. Thechunkswillbecreatedwithspecifiedtokenlimitsandoverlapsettingstooptimizeforbothcomprehensionandprocessingefficiency.Each chunk will contain metadata about its position in the original text and token count for further processing. By using a robust chunking strategy, we can ensure that downstream models receive high-quality, context-rich input, improving the overall performance of NLP pipelines and applications.\n",
      "\n",
      "  Start: 1062\n",
      "  End: 1681\n",
      "  Tokens: 577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunks = helix.Chunk.slumber_chunk(massive_text_blob)\n",
    "print_chunks(chunks, \"Slumber Chunker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cf1fdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ¦› choooooooooooooooooooonk 100% â€¢ 1/1 splits processed [00:05<00:00,  5.84s/split] ðŸŒ±\n",
      "ðŸ¦› choooooooooooooooooooonk 100% â€¢ 1/1 splits processed [00:06<00:00,  6.27s/split] ðŸŒ±\n",
      "ðŸ¦› choooooooooooooooooooonk 100% â€¢ 2/2 docs chunked [00:12<00:00,  6.06s/doc] ðŸŒ±"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Slumber Chunker - Batch Text ===\n",
      "\n",
      "Document 1 (1 chunks):\n",
      "  Chunk 1: First document to chunk with some content for testing. (tokens: 54)\n",
      "\n",
      "Document 2 (1 chunks):\n",
      "  Chunk 1: Second document with different content for batch processing. (tokens: 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_chunks = helix.Chunk.slumber_chunk(texts)\n",
    "print_batch_chunks(batch_chunks, \"Slumber Chunker\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
